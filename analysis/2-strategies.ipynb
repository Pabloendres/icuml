{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce74bfb8-3d2d-4fac-b28d-22d8da5caf90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature selection strategies\n",
    "\n",
    "In this notebook we train models using the 57 parameters available. Then, we select the most important parameters through Strategy A, B, and C.\n",
    "\n",
    "This require high computational power. The entire notebook, including model fitting and feature selection, can be run in about 20-48 hours using a 16-core CPU. Set `run = True` to run the notebook including intensive task. Otherwise, set `run = False` to use previously saved work and solely plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f29f3ec-7491-4f69-9c0c-9a5adfa0deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run intensive tasks\n",
    "run = False\n",
    "\n",
    "import aidxmods as axm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501b9f1-7cd4-43bc-b23e-a713bb0b9ec0",
   "metadata": {},
   "source": [
    "# Full parameter models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69d382a-c088-4c29-bdbd-1ac6b80f0cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter name: voyager\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 57\n",
      "Features used: 98\n",
      "MLC                   | Training AP           | Test AP               | Validation AP        \n",
      "--------------------- | --------------------- | --------------------- | ---------------------\n",
      "XGBoost               | 0.380 [0.361 - 0.398] | 0.297 [0.261 - 0.334] | 0.252 [0.239 - 0.268]\n",
      "Random Forest         | 0.298 [0.282 - 0.316] | 0.213 [0.185 - 0.245] | 0.210 [0.199 - 0.225]\n",
      "Logistic Regression   | 0.171 [0.158 - 0.184] | 0.163 [0.139 - 0.192] | 0.203 [0.191 - 0.218]\n",
      "Naive Bayes           | 0.135 [0.127 - 0.143] | 0.136 [0.119 - 0.153] | 0.151 [0.144 - 0.160]\n",
      "SOFA                  | 0.079 [0.074 - 0.083] | 0.076 [0.068 - 0.085] | 0.152 [0.144 - 0.160]\n",
      "OASIS                 | 0.134 [0.125 - 0.144] | 0.138 [0.121 - 0.162] | 0.170 [0.161 - 0.181]\n",
      "MODS                  | 0.094 [0.088 - 0.102] | 0.089 [0.078 - 0.103] | 0.152 [0.143 - 0.162]\n",
      "APACHE II             | 0.061 [0.057 - 0.065] | 0.061 [0.054 - 0.072] | 0.147 [0.140 - 0.156]\n",
      "SAPS II               | 0.175 [0.162 - 0.189] | 0.178 [0.154 - 0.206] | 0.207 [0.196 - 0.221]\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "if run:\n",
    "    voyager = (\n",
    "        axm.get(name='voyager')\n",
    "        .preprocess()\n",
    "        .fitmodel()\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    voyager = axm.load('voyager')\n",
    "\n",
    "print(voyager)\n",
    "print_ap = lambda x: f'{x[0]:.3f} [{x[1]:.3f} - {x[2]:.3f}]'\n",
    "\n",
    "voyager_train = voyager.ap_prc(dataset='train')\n",
    "voyager_test = voyager.ap_prc(dataset='test')\n",
    "voyager_val = voyager.ap_prc(dataset='val')\n",
    "print(f'{\"MLC\":21}', f'{\"Training AP\":21}', f'{\"Test AP\":21}', f'{\"Validation AP\":21}', sep=' | ')\n",
    "print(f'{\"\":-<21}', f'{\"\":-<21}', f'{\"\":-<21}', f'{\"\":-<21}', sep=' | ')\n",
    "for estimator in voyager.available_estimators.values():\n",
    "    print(f'{estimator:21}', print_ap(voyager_train[estimator]), print_ap(voyager_test[estimator]), print_ap(voyager_val[estimator]), sep=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ef125-049b-4d4d-8c97-805b5c1f3513",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Strategy A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec74dc19-8931-4820-ae98-3ab2f503176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter name: strata20xg\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 37\n",
      "\n",
      "Fitter name: strata20rf\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 19\n",
      "Features used: 37\n",
      "\n",
      "Fitter name: strata20lr\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 36\n",
      "\n",
      "Fitter name: strata20nb\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "if run:\n",
    "    strata = (\n",
    "        axm.get(name='strata')\n",
    "        .preprocess()\n",
    "        .fitmodel()\n",
    "    )\n",
    "    strata = axm.rfe(strata, n=22, step=1, n_iter=5, kfold=10).save()\n",
    "    \n",
    "else:\n",
    "    strata = axm.load('strata')\n",
    "    \n",
    "# Select most important features based on RFE\n",
    "mif = axm.most_important_features(strata)\n",
    "\n",
    "# Make a dict of fitters with most important features\n",
    "strata20 = dict()\n",
    "for key, features in mif.items():\n",
    "    if run:\n",
    "        strata20[key] = (\n",
    "            axm.get(name='strata20' + key)\n",
    "            .preprocess(restrict=features)\n",
    "            .fitmodel(key)\n",
    "            .save()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        strata20[key] = axm.load('strata20' + key)\n",
    "        \n",
    "    print(strata20[key], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2877a1-1318-4569-a9bf-7446d34c41d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Strategy B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6cd300-0be1-4557-ae9a-03e7cccb1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter name: stratb20xg\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 37\n",
      "\n",
      "Fitter name: stratb20rf\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 39\n",
      "\n",
      "Fitter name: stratb20lr\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 31\n",
      "\n",
      "Fitter name: stratb20nb\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 20\n",
      "Features used: 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "if run:\n",
    "    stratb = (\n",
    "        axm.get(name='stratb')\n",
    "        .preprocess(restrict=list(axm.get_features('base', '!missingFlag', 'enabledToFit')))\n",
    "        .fitmodel()\n",
    "        .save()\n",
    "    )\n",
    "    stratb = axm.rfe(stratb, n=22, step=1, n_iter=5, kfold=10).save()\n",
    "    \n",
    "else:\n",
    "    stratb = axm.load('stratb')\n",
    "    \n",
    "# Select most important features based on RFE\n",
    "mif = axm.most_important_features(stratb)\n",
    "\n",
    "# Make a dict of fitters with most important features\n",
    "stratb20 = dict()\n",
    "for key, features in mif.items():\n",
    "    if run:\n",
    "        stratb20[key] = (\n",
    "            axm.get(name='stratb20' + key)\n",
    "            .preprocess(restrict=features)\n",
    "            .fitmodel(key)\n",
    "            .save()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        stratb20[key] = axm.load('stratb20' + key)\n",
    "        \n",
    "    print(stratb20[key], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e34620-87ab-4231-88b5-20dad24448a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Strategy C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c5aea8-42de-42c7-9d24-ab8a5fe38dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter name: stratc\n",
      "Total dataset size: 112,023\n",
      "Training dataset size: 65,880\n",
      "Testing dataset size: 16,471\n",
      "Validation dataset size: 29,672\n",
      "Parameters used: 19\n",
      "Features used: 30\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "if run:\n",
    "    stratc = (\n",
    "        axm.get(name='stratc')\n",
    "        .preprocess(restrict=[\"Age\", \"Age Miss\" \"Heart rate\", \"Heart rate Miss\", \"SBP\", \"SBP Miss\", \n",
    "                              \"Temperature\", \"Temperatur Miss\", \"GCS\", \"GCS Miss\", \"PAO2\", \"PAO2 Miss\", \n",
    "                              \"FIO2\", \"FIO2 Miss\", \"BUN\", \"BUN Miss\", \"Urine output\", \"Urine Output Miss\", \n",
    "                              \"Sodium\", \"Sodium Miss\", \"Potassium\", \"Potassium Miss\", \"Bicarbonate\", \"Bicarbonate Miss\", \n",
    "                              \"Bilirubin\", \"Bilirubin Miss\", \"WBC\", \"WBC Miss\", \"Ventilation\", \"Cancer\", \n",
    "                              \"Lymphoma\", \"AIDS\", \"Admission for surgery\", \"Adm. for elec. surgery\"])\n",
    "        .fitmodel()\n",
    "        .save()\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    stratc = axm.load('stratc')\n",
    "    \n",
    "print(stratc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ef2c4-b181-4463-ac69-6dbc68ece97a",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b99d2-1308-4ec0-a068-b7b033b2bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strata_fimp = pd.DataFrame([\n",
    "    strata20['xg'].get_feature_importance('xg'),\n",
    "    strata20['rf'].get_feature_importance('rf'),\n",
    "    strata20['lr'].get_feature_importance('lr'),\n",
    "    strata20['nb'].get_feature_importance('nb')\n",
    "])\n",
    "\n",
    "stratb_fimp = pd.DataFrame([\n",
    "    stratb20['xg'].get_feature_importance('xg'),\n",
    "    stratb20['rf'].get_feature_importance('rf'),\n",
    "    stratb20['lr'].get_feature_importance('lr'),\n",
    "    stratb20['nb'].get_feature_importance('nb')\n",
    "])\n",
    "\n",
    "stratc_fimp = pd.DataFrame([\n",
    "    stratc.get_feature_importance('xg'),\n",
    "    stratc.get_feature_importance('rf'),\n",
    "    stratc.get_feature_importance('lr'),\n",
    "    stratc.get_feature_importance('nb')\n",
    "])\n",
    "\n",
    "d = {'Strategy A' : strata_fimp, 'Strategy B' : stratb_fimp, 'Strategy C': stratc_fimp}\n",
    "summary_fimp = pd.concat(d.values(), axis=0, keys=d.keys()).T\n",
    "summary_fimp.to_csv('data/summary-fimportances.csv')\n",
    "print('feature importance summary')\n",
    "summary_fimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737bda9-b6e3-483b-80f6-4c9974800a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc = ['xg', 'rf', 'lr', 'nb']\n",
    "MLC = ['XGBoost', 'Random Forest', 'Logistic Regression', 'Naive Bayes']\n",
    "cus = ['saps', 'apache', 'mods', 'oasis']\n",
    "CUS = ['SAPS II', 'APACHE II', 'MODS', 'OASIS']\n",
    "\n",
    "a1, a2, a3 = dict(), dict(), dict()\n",
    "b1, b2, b3 = dict(), dict(), dict()\n",
    "c1, c2, c3 = dict(), dict(), dict()\n",
    "\n",
    "for key in mlc:\n",
    "    estimator_name = axm.Fitter.available_estimators[key]\n",
    "    \n",
    "    fitter = axm.load('strata20' + key)\n",
    "    a1[estimator_name] = fitter.ap_prc(key, 'train')[estimator_name]\n",
    "    a2[estimator_name] = fitter.ap_prc(key, 'test')[estimator_name]\n",
    "    a3[estimator_name] = fitter.ap_prc(key, 'val')[estimator_name]\n",
    "    \n",
    "    fitter = axm.load('stratb20' + key)\n",
    "    b1[estimator_name] = fitter.ap_prc(key, 'train')[estimator_name]\n",
    "    b2[estimator_name] = fitter.ap_prc(key, 'test')[estimator_name]\n",
    "    b3[estimator_name] = fitter.ap_prc(key, 'val')[estimator_name]\n",
    "\n",
    "stratc = axm.load('stratc')\n",
    "c1 = stratc.ap_prc(mlc + cus, 'train')\n",
    "c2 = stratc.ap_prc(mlc + cus, 'test')\n",
    "c3 = stratc.ap_prc(mlc + cus, 'val')\n",
    "\n",
    "for key in cus:\n",
    "    estimator_name = stratc.available_estimators[key]\n",
    "    \n",
    "    a1[estimator_name] = c1[estimator_name]\n",
    "    a2[estimator_name] = c2[estimator_name]\n",
    "    a3[estimator_name] = c3[estimator_name]\n",
    "    \n",
    "    b1[estimator_name] = c1[estimator_name]\n",
    "    b2[estimator_name] = c2[estimator_name]\n",
    "    b3[estimator_name] = c3[estimator_name]\n",
    "    \n",
    "columns = [\"Training\", \"Test\", \"Validation\"]\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        [\"Strategy A\", \"XGBoost\"],\n",
    "        [\"Strategy A\", \"Random Forest\"],\n",
    "        [\"Strategy A\", \"Logistic Regression\"],\n",
    "        [\"Strategy A\", \"Naive Bayes\"],\n",
    "        [\"Strategy B\", \"XGBoost\"],\n",
    "        [\"Strategy B\", \"Random Forest\"],\n",
    "        [\"Strategy B\", \"Logistic Regression\"],\n",
    "        [\"Strategy B\", \"Naive Bayes\"],\n",
    "        [\"Strategy C\", \"XGBoost\"],\n",
    "        [\"Strategy C\", \"Random Forest\"],\n",
    "        [\"Strategy C\", \"Logistic Regression\"],\n",
    "        [\"Strategy C\", \"Naive Bayes\"],\n",
    "        [\"CUS\", \"SAPS II\"],\n",
    "        [\"CUS\", \"APACHE II\"],\n",
    "        [\"CUS\", \"MODS\"],\n",
    "        [\"CUS\", \"OASIS\"],\n",
    "    ], names=[\"Group\", \"Estimator\"])\n",
    "summary_perf = pd.DataFrame(columns=columns, index=index)\n",
    "print_ap = lambda x: f'{x[0]:.3f} [{x[1]:.3f} - {x[2]:.3f}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073f1c3-82bc-48f8-ad65-9d95d4e1815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = MLC + CUS\n",
    "groups = ['Training', 'Test', 'Validation']\n",
    "N = len(estimators)\n",
    "width = 0.8\n",
    "color_cycle = axm.GS_color_cycle[:4] * 2\n",
    "hatch_cycle = [''] * 4 + ['///'] * 4\n",
    "    \n",
    "fig, ax = axm.setfig(1, 1, nseries=len(estimators), figsize=(7, 3))\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    offset = axm.get_barplot_offset(i, N)\n",
    "    xpos = np.arange(len(groups)) + width * offset\n",
    "    \n",
    "    mean1, lower1, upper1 = a1[estimator]\n",
    "    mean2, lower2, upper2 = a2[estimator]\n",
    "    mean3, lower3, upper3 = a3[estimator]\n",
    "    yerr = [[mean1 - lower1, mean2 - lower2, mean3 - lower3], [upper1 - mean1, upper2 - mean2, upper3 - mean3]]\n",
    "    \n",
    "    ax.bar(\n",
    "        xpos, [mean1, mean2, mean3], width / N, \n",
    "        yerr=yerr, capsize=3, label=estimator, \n",
    "        color=color_cycle[i], edgecolor='#ffffff', hatch=hatch_cycle[i]\n",
    "    )\n",
    "    ax.bar(xpos, [mean1, mean2, mean3], width / N, color='none', edgecolor='k', zorder=1)\n",
    "    ax.set_ylabel('Average precision')\n",
    "    ax.set_ylim(0, 0.45)\n",
    "    ax.set_xticks(np.arange(len(groups)))\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.grid(True, axis='y')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1, 0, 0))\n",
    "    \n",
    "    if estimator in MLC:\n",
    "        summary_perf.loc[('Strategy A', estimator)] = (\n",
    "            print_ap(a1[estimator]),\n",
    "            print_ap(a2[estimator]),\n",
    "            print_ap(a3[estimator])\n",
    "        )\n",
    "    \n",
    "plt.tight_layout()\n",
    "axm.handlefig('2-performance-summary-strata')\n",
    "\n",
    "fig, ax = axm.setfig(1, 1, nseries=len(estimators), figsize=(7, 3))\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    offset = axm.get_barplot_offset(i, N)\n",
    "    xpos = np.arange(len(groups)) + width * offset\n",
    "    \n",
    "    mean1, lower1, upper1 = b1[estimator]\n",
    "    mean2, lower2, upper2 = b2[estimator]\n",
    "    mean3, lower3, upper3 = b3[estimator]\n",
    "    yerr = [[mean1 - lower1, mean2 - lower2, mean3 - lower3], [upper1 - mean1, upper2 - mean2, upper3 - mean3]]\n",
    "    \n",
    "    ax.bar(\n",
    "        xpos, [mean1, mean2, mean3], width / N, \n",
    "        yerr=yerr, capsize=3, label=estimator, \n",
    "        color=color_cycle[i], edgecolor='#ffffff', hatch=hatch_cycle[i]\n",
    "    )\n",
    "    ax.bar(xpos, [mean1, mean2, mean3], width / N, color='none', edgecolor='k', zorder=1)\n",
    "    ax.set_ylabel('Average precision')\n",
    "    ax.set_ylim(0, 0.45)\n",
    "    ax.set_xticks(np.arange(len(groups)))\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.grid(True, axis='y')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1, 0, 0))\n",
    "    \n",
    "    if estimator in MLC:\n",
    "        summary_perf.loc[('Strategy B', estimator)] = (\n",
    "            print_ap(b1[estimator]),\n",
    "            print_ap(b2[estimator]),\n",
    "            print_ap(b3[estimator])\n",
    "        )\n",
    "    \n",
    "plt.tight_layout()\n",
    "axm.handlefig('2-performance-summary-stratb')\n",
    "\n",
    "fig, ax = axm.setfig(1, 1, nseries=len(estimators), figsize=(7, 3))\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    offset = axm.get_barplot_offset(i, N)\n",
    "    xpos = np.arange(len(groups)) + width * offset\n",
    "    \n",
    "    mean1, lower1, upper1 = c1[estimator]\n",
    "    mean2, lower2, upper2 = c2[estimator]\n",
    "    mean3, lower3, upper3 = c3[estimator]\n",
    "    yerr = [[mean1 - lower1, mean2 - lower2, mean3 - lower3], [upper1 - mean1, upper2 - mean2, upper3 - mean3]]\n",
    "    \n",
    "    ax.bar(\n",
    "        xpos, [mean1, mean2, mean3], width / N, \n",
    "        yerr=yerr, capsize=3, label=estimator, \n",
    "        color=color_cycle[i], edgecolor='#ffffff', hatch=hatch_cycle[i]\n",
    "    )\n",
    "    ax.bar(xpos, [mean1, mean2, mean3], width / N, color='none', edgecolor='k', zorder=1)\n",
    "    ax.set_ylabel('Average precision')\n",
    "    ax.set_ylim(0, 0.45)\n",
    "    ax.set_xticks(np.arange(len(groups)))\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.grid(True, axis='y')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1, 0, 0))\n",
    "    \n",
    "    if estimator in MLC:\n",
    "        summary_perf.loc[('Strategy C', estimator)] = (\n",
    "            print_ap(c1[estimator]),\n",
    "            print_ap(c2[estimator]),\n",
    "            print_ap(c3[estimator])\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        summary_perf.loc[('CUS', estimator)] = (\n",
    "            print_ap(c1[estimator]),\n",
    "            print_ap(c2[estimator]),\n",
    "            print_ap(c3[estimator])\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "axm.handlefig('2-performance-summary-stratc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f6277-c525-459d-a3ed-88f8ff6a9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_perf.to_csv('data/summary-performances.csv')\n",
    "summary_perf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
